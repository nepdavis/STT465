theta=exp(Xb)/(1+exp(Xb))
logLik=sum( dbinom(x=y,p=theta,size=1,log=T)  )
logPrior=sum(  dnorm(x=b,sd=sqrt(varB),mean=b0,log=T))
return(logLik+logPrior)
}
logisticRegressionBayes=function(y,X,nIter=12000,V=.02,varB=rep(10000,ncol(X)),b0=rep(0,ncol(X))){
####### Arguments #######################
# y  a vector with 0/1 values
# X  incidence matrix fo effects
# b0,varB, the prior mean and prior variance bj~N(b0[j],varB[j])
# V the variance of the normal distribution used to generate candidates~N(b[i-1],V)
# nIter: number of iterations of the sampler
# Details: generates samples from the posterior distribution of a logistic regression using a Metropolis algorithm
#########################################
# A matrix to store samples
p=ncol(X)
B=matrix(nrow=nIter,ncol=p)
# Centering predictors
meanX=colMeans(X)
for(i in 2:p){ X[,i]=(X[,i]-meanX[i]) }
# A vector to trace acceptancve
accept=rep(NA,nIter)
accept[1]=TRUE
# Initialize
B[1,]=0
B[1,1]=log(mean(y)/(1-mean(y)))
b=B[1,]
for(i in 2:nIter){
candidate=rnorm(mean=b,sd=sqrt(V),n=p)
logP_current=logP(y,X,b0=b0,varB=varB,b=b)
logP_candidate=logP(y,X,b0=b0,varB=varB,b=candidate)
r=min(1,exp(logP_candidate-logP_current))
delta=rbinom(n=1,size=1,p=r)
accept[i]=delta
if(delta==1){ b=candidate }
B[i,]=b
print(paste0(i," accept?=",delta==1))
}
B[,1]=B[,1]-B[,-1]%*%meanX[-1] # absorbing means on the intercept
return(list(B=B,accept=accept))
}
# A function to evaluate the log of the posterior density
logP=function(y,X,b,b0,varB){
Xb=X%*%b
theta=exp(Xb)/(1+exp(Xb))
logLik=sum( dbinom(x=y,p=theta,size=1,log=T)  )
logPrior=sum(  dnorm(x=b,sd=sqrt(varB),mean=b0,log=T))
return(logLik+logPrior)
}
logisticRegressionBayes=function(y,X,nIter=12000,V=.02,varB=rep(10000,ncol(X)),b0=rep(0,ncol(X))){
####### Arguments #######################
# y  a vector with 0/1 values
# X  incidence matrix fo effects
# b0,varB, the prior mean and prior variance bj~N(b0[j],varB[j])
# V the variance of the normal distribution used to generate candidates~N(b[i-1],V)
# nIter: number of iterations of the sampler
# Details: generates samples from the posterior distribution of a logistic regression using a Metropolis algorithm
#########################################
# A matrix to store samples
p=ncol(X)
B=matrix(nrow=nIter,ncol=p)
# Centering predictors
meanX=colMeans(X)
for(i in 2:p){ X[,i]=(X[,i]-meanX[i]) }
# A vector to trace acceptancve
accept=rep(NA,nIter)
accept[1]=TRUE
# Initialize
B[1,]=0
B[1,1]=log(mean(y)/(1-mean(y)))
b=B[1,]
for(i in 2:nIter){
candidate=rnorm(mean=b,sd=sqrt(V),n=p)
logP_current=logP(y,X,b0=b0,varB=varB,b=b)
logP_candidate=logP(y,X,b0=b0,varB=varB,b=candidate)
r=min(1,exp(logP_candidate-logP_current))
delta=rbinom(n=1,size=1,p=r)
accept[i]=delta
if(delta==1){ b=candidate }
B[i,]=b
print(paste0(i," accept?=",delta==1))
}
B[,1]=B[,1]-B[,-1]%*%meanX[-1] # absorbing means on the intercept
return(list(B=B,accept=accept))
}
b_model <- logisticRegressionBayes(data$gout, x, 55000)
cbind(model$coef,colMeans(b_model$B[-(1:5000),]))
model$coefficients
cbind(model$coefficients,colMeans(b_model$B[-(1:5000),]))
result <- cbind(model$coefficients,colMeans(b_model$B[-(1:5000),]))
View(result)
# A function to evaluate the log of the posterior density
logP=function(y,X,b,b0,varB){
Xb=X%*%b
theta=exp(Xb)/(1+exp(Xb))
logLik=sum( dbinom(x=y,p=theta,size=1,log=T)  )
logPrior=sum(  dnorm(x=b,sd=sqrt(varB),mean=b0,log=T))
return(logLik+logPrior)
}
logisticRegressionBayes=function(y,X,nIter=12000,V=.02,varB=rep(10000,ncol(X)),b0=rep(0,ncol(X))){
####### Arguments #######################
# y  a vector with 0/1 values
# X  incidence matrix fo effects
# b0,varB, the prior mean and prior variance bj~N(b0[j],varB[j])
# V the variance of the normal distribution used to generate candidates~N(b[i-1],V)
# nIter: number of iterations of the sampler
# Details: generates samples from the posterior distribution of a logistic regression using a Metropolis algorithm
#########################################
# A matrix to store samples
p=ncol(X)
B=matrix(nrow=nIter,ncol=p)
# Centering predictors
meanX=colMeans(X)
for(i in 2:p){ X[,i]=(X[,i]-meanX[i]) }
# A vector to trace acceptancve
accept=rep(NA,nIter)
accept[1]=TRUE
# Initialize
B[1,]=0
B[1,1]=log(mean(y)/(1-mean(y)))
b=B[1,]
for(i in 2:nIter){
candidate=rnorm(mean=b,sd=sqrt(V),n=p)
logP_current=logP(y,X,b0=b0,varB=varB,b=b)
logP_candidate=logP(y,X,b0=b0,varB=varB,b=candidate)
r=min(1,exp(logP_candidate-logP_current))
delta=rbinom(n=1,size=1,p=r)
accept[i]=delta
if(delta==1){ b=candidate }
B[i,]=b
}
B[,1]=B[,1]-B[,-1]%*%meanX[-1] # absorbing means on the intercept
return(list(B=B,accept=accept))
}
b_model <- logisticRegressionBayes(data$gout, model.matrix(~sex+race+age,data=data), 10000)
result <- cbind(model$coefficients,colMeans(b_model$B[-(1:1000),]))
View(result)
# A function to evaluate the log of the posterior density
logP=function(y,X,b,b0,varB){
Xb=X%*%b
theta=exp(Xb)/(1+exp(Xb))
logLik=sum( dbinom(x=y,p=theta,size=1,log=T)  )
logPrior=sum(  dnorm(x=b,sd=sqrt(varB),mean=b0,log=T))
return(logLik+logPrior)
}
logisticRegressionBayes=function(y,X,nIter=12000,V=.02,varB=rep(10000,ncol(X)),b0=rep(0,ncol(X))){
####### Arguments #######################
# y  a vector with 0/1 values
# X  incidence matrix fo effects
# b0,varB, the prior mean and prior variance bj~N(b0[j],varB[j])
# V the variance of the normal distribution used to generate candidates~N(b[i-1],V)
# nIter: number of iterations of the sampler
# Details: generates samples from the posterior distribution of a logistic regression using a Metropolis algorithm
#########################################
# A matrix to store samples
p=ncol(X)
B=matrix(nrow=nIter,ncol=p)
# Centering predictors
meanX=colMeans(X)
for(i in 2:p){ X[,i]=(X[,i]-meanX[i]) }
# A vector to trace acceptancve
accept=rep(NA,nIter)
accept[1]=TRUE
# Initialize
B[1,]=0
B[1,1]=log(mean(y)/(1-mean(y)))
b=B[1,]
for(i in 2:nIter){
candidate=rnorm(mean=b,sd=sqrt(V),n=p)
logP_current=logP(y,X,b0=b0,varB=varB,b=b)
logP_candidate=logP(y,X,b0=b0,varB=varB,b=candidate)
r=min(1,exp(logP_candidate-logP_current))
delta=rbinom(n=1,size=1,p=r)
accept[i]=delta
if(delta==1){ b=candidate }
B[i,]=b
}
B[,1]=B[,1]-B[,-1]%*%meanX[-1] # absorbing means on the intercept
return(list(B=B,accept=accept))
}
b_model <- logisticRegressionBayes(data$gout, model.matrix(~sex+age+race,data=data), 10000)
result <- cbind(model$coefficients,colMeans(b_model$B[-(1:1000),]))
View(result)
# A function to evaluate the log of the posterior density
logP=function(y,X,b,b0,varB){
Xb=X%*%b
theta=exp(Xb)/(1+exp(Xb))
logLik=sum( dbinom(x=y,p=theta,size=1,log=T)  )
logPrior=sum(  dnorm(x=b,sd=sqrt(varB),mean=b0,log=T))
return(logLik+logPrior)
}
logisticRegressionBayes=function(y,X,nIter=12000,V=.02,varB=rep(10000,ncol(X)),b0=rep(0,ncol(X))){
####### Arguments #######################
# y  a vector with 0/1 values
# X  incidence matrix fo effects
# b0,varB, the prior mean and prior variance bj~N(b0[j],varB[j])
# V the variance of the normal distribution used to generate candidates~N(b[i-1],V)
# nIter: number of iterations of the sampler
# Details: generates samples from the posterior distribution of a logistic regression using a Metropolis algorithm
#########################################
# A matrix to store samples
p=ncol(X)
B=matrix(nrow=nIter,ncol=p)
# Centering predictors
meanX=colMeans(X)
for(i in 2:p){ X[,i]=(X[,i]-meanX[i]) }
# A vector to trace acceptancve
accept=rep(NA,nIter)
accept[1]=TRUE
# Initialize
B[1,]=0
B[1,1]=log(mean(y)/(1-mean(y)))
b=B[1,]
for(i in 2:nIter){
candidate=rnorm(mean=b,sd=sqrt(V),n=p)
logP_current=logP(y,X,b0=b0,varB=varB,b=b)
logP_candidate=logP(y,X,b0=b0,varB=varB,b=candidate)
r=min(1,exp(logP_candidate-logP_current))
delta=rbinom(n=1,size=1,p=r)
accept[i]=delta
if(delta==1){ b=candidate }
B[i,]=b
}
B[,1]=B[,1]-B[,-1]%*%meanX[-1] # absorbing means on the intercept
return(list(B=B,accept=accept))
}
b_model <- logisticRegressionBayes(data$gout, model.matrix(~sex+age+race,data=data), 20000)
result <- cbind(model$coefficients,colMeans(b_model$B[-(1:5000),]))
View(result)
str(b_model)
plot(b_model$B)
values <- b_model$B
View(values)
plot(values[,1])
# A function to evaluate the log of the posterior density
logP=function(y,X,b,b0,varB){
Xb=X%*%b
theta=exp(Xb)/(1+exp(Xb))
logLik=sum( dbinom(x=y,p=theta,size=1,log=T)  )
logPrior=sum(  dnorm(x=b,sd=sqrt(varB),mean=b0,log=T))
return(logLik+logPrior)
}
logisticRegressionBayes=function(y,X,nIter=12000,V=.02,varB=rep(10000,ncol(X)),b0=rep(0,ncol(X))){
####### Arguments #######################
# y  a vector with 0/1 values
# X  incidence matrix fo effects
# b0,varB, the prior mean and prior variance bj~N(b0[j],varB[j])
# V the variance of the normal distribution used to generate candidates~N(b[i-1],V)
# nIter: number of iterations of the sampler
# Details: generates samples from the posterior distribution of a logistic regression using a Metropolis algorithm
#########################################
# A matrix to store samples
p=ncol(X)
B=matrix(nrow=nIter,ncol=p)
# Centering predictors
meanX=colMeans(X)
for(i in 2:p){ X[,i]=(X[,i]-meanX[i]) }
# A vector to trace acceptancve
accept=rep(NA,nIter)
accept[1]=TRUE
# Initialize
B[1,]=0
B[1,1]=log(mean(y)/(1-mean(y)))
b=B[1,]
for(i in 2:nIter){
candidate=rnorm(mean=b,sd=sqrt(V),n=p)
logP_current=logP(y,X,b0=b0,varB=varB,b=b)
logP_candidate=logP(y,X,b0=b0,varB=varB,b=candidate)
r=min(1,exp(logP_candidate-logP_current))
delta=rbinom(n=1,size=1,p=r)
accept[i]=delta
if(delta==1){ b=candidate }
B[i,]=b
}
B[,1]=B[,1]-B[,-1]%*%meanX[-1] # absorbing means on the intercept
return(list(B=B,accept=accept))
}
b_model <- logisticRegressionBayes(data$gout, model.matrix(~sex+age+race,data=data), 55000)
result <- cbind(model$coefficients,colMeans(b_model$B[-(1:5000),]))
View(result)
model.matrix(~sex+age+race,data=data)
# A function to evaluate the log of the posterior density
logP=function(y,X,b,b0,varB){
Xb=X%*%b
theta=exp(Xb)/(1+exp(Xb))
logLik=sum( dbinom(x=y,p=theta,size=1,log=T)  )
logPrior=sum(  dnorm(x=b,sd=sqrt(varB),mean=b0,log=T))
return(logLik+logPrior)
}
logisticRegressionBayes=function(y,X,nIter=12000,V=.02,varB=rep(10000,ncol(X)),b0=rep(0,ncol(X))){
####### Arguments #######################
# y  a vector with 0/1 values
# X  incidence matrix fo effects
# b0,varB, the prior mean and prior variance bj~N(b0[j],varB[j])
# V the variance of the normal distribution used to generate candidates~N(b[i-1],V)
# nIter: number of iterations of the sampler
# Details: generates samples from the posterior distribution of a logistic regression using a Metropolis algorithm
#########################################
# A matrix to store samples
p=ncol(X)
B=matrix(nrow=nIter,ncol=p)
# Centering predictors
meanX=colMeans(X)
for(i in 2:p){ X[,i]=(X[,i]-meanX[i]) }
# A vector to trace acceptancve
accept=rep(NA,nIter)
accept[1]=TRUE
# Initialize
B[1,]=0
B[1,1]=log(mean(y)/(1-mean(y)))
b=B[1,]
for(i in 2:nIter){
candidate=rnorm(mean=b,sd=sqrt(V),n=p)
logP_current=logP(y,X,b0=b0,varB=varB,b=b)
logP_candidate=logP(y,X,b0=b0,varB=varB,b=candidate)
r=min(1,exp(logP_candidate-logP_current))
delta=rbinom(n=1,size=1,p=r)
accept[i]=delta
if(delta==1){ b=candidate }
B[i,]=b
}
B[,1]=B[,1]-B[,-1]%*%meanX[-1] # absorbing means on the intercept
return(list(B=B,accept=accept))
}
b_model <- logisticRegressionBayes(data$gout, model.matrix(~sex+age+race,data=data), 55000)
result <- cbind(model$coefficients,colMeans(b_model$B[-(1:5000),]))
density(b_model$B[,1])
plot(density(b_model$B[,1]))
# A function to evaluate the log of the posterior density
logP=function(y,X,b,b0,varB){
Xb=X%*%b
theta=exp(Xb)/(1+exp(Xb))
logLik=sum( dbinom(x=y,p=theta,size=1,log=T)  )
logPrior=sum(  dnorm(x=b,sd=sqrt(varB),mean=b0,log=T))
return(logLik+logPrior)
}
logisticRegressionBayes=function(y,X,nIter=12000,V=.02,varB=rep(10000,ncol(X)),b0=rep(0,ncol(X))){
####### Arguments #######################
# y  a vector with 0/1 values
# X  incidence matrix fo effects
# b0,varB, the prior mean and prior variance bj~N(b0[j],varB[j])
# V the variance of the normal distribution used to generate candidates~N(b[i-1],V)
# nIter: number of iterations of the sampler
# Details: generates samples from the posterior distribution of a logistic regression using a Metropolis algorithm
#########################################
# A matrix to store samples
p=ncol(X)
B=matrix(nrow=nIter,ncol=p)
# Centering predictors
meanX=colMeans(X)
for(i in 2:p){ X[,i]=(X[,i]-meanX[i]) }
# A vector to trace acceptancve
accept=rep(NA,nIter)
accept[1]=TRUE
# Initialize
B[1,]=0
B[1,1]=log(mean(y)/(1-mean(y)))
b=B[1,]
for(i in 2:nIter){
candidate=rnorm(mean=b,sd=sqrt(V),n=p)
logP_current=logP(y,X,b0=b0,varB=varB,b=b)
logP_candidate=logP(y,X,b0=b0,varB=varB,b=candidate)
r=min(1,exp(logP_candidate-logP_current))
delta=rbinom(n=1,size=1,p=r)
accept[i]=delta
if(delta==1){ b=candidate }
B[i,]=b
}
B[,1]=B[,1]-B[,-1]%*%meanX[-1] # absorbing means on the intercept
return(list(B=B,accept=accept))
}
b_model <- logisticRegressionBayes(data$gout, model.matrix(~sex+age+race,data=data), 55000)
result <- cbind(model$coefficients,colMeans(b_model$B[-(1:5000),]))
params <- b_model$B
colnames(params) <- c("Intercept", "Sex", "Age", "Race")
mcmc_obj <- mcmc(params)
library("coda")
mcmc_obj <- mcmc(params)
obj_sum <- summary(mcmc_obj)
library("coda")
mcmc_obj <- mcmc(params)
obj_sum <- summary(mcmc_obj)
obj_sum
# A function to evaluate the log of the posterior density
logP=function(y,X,b,b0,varB){
Xb=X%*%b
theta=exp(Xb)/(1+exp(Xb))
logLik=sum( dbinom(x=y,p=theta,size=1,log=T)  )
logPrior=sum(  dnorm(x=b,sd=sqrt(varB),mean=b0,log=T))
return(logLik+logPrior)
}
logisticRegressionBayes=function(y,X,nIter=12000,V=.02,varB=rep(10000,ncol(X)),b0=rep(0,ncol(X))){
####### Arguments #######################
# y  a vector with 0/1 values
# X  incidence matrix fo effects
# b0,varB, the prior mean and prior variance bj~N(b0[j],varB[j])
# V the variance of the normal distribution used to generate candidates~N(b[i-1],V)
# nIter: number of iterations of the sampler
# Details: generates samples from the posterior distribution of a logistic regression using a Metropolis algorithm
#########################################
# A matrix to store samples
p=ncol(X)
B=matrix(nrow=nIter,ncol=p)
# Centering predictors
meanX=colMeans(X)
for(i in 2:p){ X[,i]=(X[,i]-meanX[i]) }
# A vector to trace acceptancve
accept=rep(NA,nIter)
accept[1]=TRUE
# Initialize
B[1,]=0
B[1,1]=log(mean(y)/(1-mean(y)))
b=B[1,]
for(i in 2:nIter){
candidate=rnorm(mean=b,sd=sqrt(V),n=p)
logP_current=logP(y,X,b0=b0,varB=varB,b=b)
logP_candidate=logP(y,X,b0=b0,varB=varB,b=candidate)
r=min(1,exp(logP_candidate-logP_current))
delta=rbinom(n=1,size=1,p=r)
accept[i]=delta
if(delta==1){ b=candidate }
B[i,]=b
}
B[,1]=B[,1]-B[,-1]%*%meanX[-1] # absorbing means on the intercept
return(list(B=B,accept=accept))
}
b_model <- logisticRegressionBayes(data$gout, model.matrix(~sex+age+race,data=data), 55000)
result <- cbind(model$coefficients,colMeans(b_model$B[-(1:5000),]))
params <- b_model$B[-(1:5000),]
colnames(params) <- c("Intercept", "Sex", "Age", "Race")
library("coda")
mcmc_obj <- mcmc(params)
obj_sum <- summary(mcmc_obj)
obj_sum
par(mfrow = c(2, 2))
plot(params$Intercept, xlab = "Index", ylab = "Est", main = "Intercept")
View(params)
params <- data.frame(params)
par(mfrow = c(2, 2))
plot(params$Intercept, xlab = "Index", ylab = "Est", main = "Intercept")
plot(params$Sex, xlab = "Index", ylab = "Est", main = "Sex")
plot(params$Age, xlab = "Index", ylab = "Est", main = "Age")
plot(params$Race, xlab = "Index", ylab = "Est", main = "Race")
par(mfrow = c(2, 2))
plot(params$Intercept, xlab = "Index", ylab = "Est", main = "Intercept", type = "l")
plot(params$Sex, xlab = "Index", ylab = "Est", main = "Sex", type = "l")
plot(params$Age, xlab = "Index", ylab = "Est", main = "Age", type = "l")
plot(params$Race, xlab = "Index", ylab = "Est", main = "Race", type = "l")
library("coda")
params <- data.frame(params)
mcmc_obj <- mcmc(params)
obj_sum <- summary(mcmc_obj)
obj_sum
par(mfrow = c(2, 2))
plot(params$Intercept, xlab = "Index", ylab = "Est", main = "Intercept", type = "l")
plot(params$Sex, xlab = "Index", ylab = "Est", main = "Sex", type = "l")
plot(params$Age, xlab = "Index", ylab = "Est", main = "Age", type = "l")
plot(params$Race, xlab = "Index", ylab = "Est", main = "Race", type = "l")
cbind(effectiveSize(mcmc_obj), obj_sum$statistics[,2])
?cbind
?data.frame
par(mfrow = c(2, 2))
plot(params$Intercept, xlab = "Index", ylab = "Est", main = "Intercept", type = "l")
plot(params$Sex, xlab = "Index", ylab = "Est", main = "Sex", type = "l")
plot(params$Age, xlab = "Index", ylab = "Est", main = "Age", type = "l")
plot(params$Race, xlab = "Index", ylab = "Est", main = "Race", type = "l")
data.frame(effectiveSize(mcmc_obj), obj_sum$statistics[,2])
par(mfrow = c(2, 2))
plot(params$Intercept, xlab = "Index", ylab = "Est", main = "Intercept", type = "l")
plot(params$Sex, xlab = "Index", ylab = "Est", main = "Sex", type = "l")
plot(params$Age, xlab = "Index", ylab = "Est", main = "Age", type = "l")
plot(params$Race, xlab = "Index", ylab = "Est", main = "Race", type = "l")
size_se <- cbind(effectiveSize(mcmc_obj), obj_sum$statistics[,3])
colnames(size_se) <- c("Effective Size", "MC SE")
size_se
?coda
traceplot(mcmc_obj)
cbind(rep(NA, 3))
cbind(rep(rep(NA, 4)), 3)
cbind(rep(rep(NA, 4), 3))
cbind(rep(rep(NA, 4)), NA, NA)
v_values <- c(.4, .2, .05, .001)
res <- cbind(rep(rep(NA, 4)), NA, NA)
row.names(res) <- v_values
res
res <- cbind(rep(rep(NA, 4)), NA, NA)
row.names(res) <- v_values
colnames(res) <- c("Acc Rate", "Lag Corr", "Effective Size")
res
autocorr(mcmc_obj)
?autocorr
autocorr(mcmc_obj$Age, lags = 100)
autocorr(mcmc_obj[3], lags = 100)
autocorr(mcmc_obj[,3], lags = 100)
res[1, 1] <- autocorr(mcmc_obj[,3], lags = 100)
res
effectiveSize(mcmc_obj)
v_values <- c(.4, .2, .05, .001)
res <- cbind(rep(rep(NA, 4)), NA, NA)
row.names(res) <- v_values
colnames(res) <- c("Acc Rate", "Lag Corr", "Effective Size")
for(i in 1:length(v_values)){
b_model <- logisticRegressionBayes(data$gout, model.matrix(~sex+age+race,data=data),
55000, V = v_values[i])
params <- b_model$B[-(1:5000),]
colnames(params) <- c("Intercept", "Sex", "Age", "Race")
res[i, 1] <- sum(b_model$accept)/length(b_model$accept)
obj <- mcmc(params)
res[i, 2] <- autocorr(obj[,3], lags = 100)
res[i, 3] <- effectiveSize(obj)[3]
}
res
